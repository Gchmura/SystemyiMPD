niestet
y
cos sie zepsu≈Ço
library('ahp'
)
library('ahp')
AND <- c(rep(0,7),1)
OR <- c(0,rep(1,7))
binary.data <- data.frame(expand.grid(c(0,1), c(0,1), c(0,1)), AND, OR)
print(net <- neuralnet(AND+OR~Var1+Var2+Var3, binary.data, hidden=0,
rep=10, err.fct="ce", linear.output=FALSE))
XOR <- c(0,1,1,0)
xor.data <- data.frame(expand.grid(c(0,1), c(0,1)), XOR)
print(net.xor <- neuralnet(XOR~Var1+Var2, xor.data, hidden=2, rep=5))
plot(net.xor, rep="best")
data(infert, package="datasets")
print(net.infert <- neuralnet(case~parity+induced+spontaneous, infert,
err.fct="ce", linear.output=FALSE, likelihood=TRUE))
gwplot(net.infert, selected.covariate="parity")
gwplot(net.infert, selected.covariate="induced")
gwplot(net.infert, selected.covariate="spontaneous")
confidence.interval(net.infert)
Var1 <- runif(50, 0, 100)
sqrt.data <- data.frame(Var1, Sqrt=sqrt(Var1))
print(net.sqrt <- neuralnet(Sqrt~Var1, sqrt.data, hidden=10,
threshold=0.01))
compute(net.sqrt, (1:10)^2)$net.result
Var1 <- rpois(100,0.5)
Var2 <- rbinom(100,2,0.6)
Var3 <- rbinom(100,1,0.5)
SUM <- as.integer(abs(Var1+Var2+Var3+(rnorm(100))))
sum.data <- data.frame(Var1+Var2+Var3, SUM)
print(net.sum <- neuralnet(SUM~Var1+Var2+Var3, sum.data, hidden=1,
act.fct="tanh"))
prediction(net.sum)
neuralnet-package
install.packages(neuralnet-package)
install.packages('neuralnet-package')
update('R')
# installing/loading the package:
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
updateR() # this will start the updating process of your R installation.  It will check for newer versions, and if one is available, will guide you through the decisions you'd need to make.
# installing/loading the package:
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
updateR() # this will start the updating process of your R installation.  It will check for newer versions, and if one is available, will guide you through the decisions you'd need to make.
# installing/loading the package:
if(!require(installr)) { install.packages("installr"); require(installr)} #load / install+load installr
updateR(F, T, T, F, T, F, T) # install, move, update.package, quit R.
# installing/loading the package:
if(!require(installr)) { install.packages("installr"); require(installr)} #load / install+load installr
# step by step functions:
check.for.updates.R() # tells you if there is a new version of R or not.
install.R() # download and run the latest R installer
copy.packages.between.libraries() # copy your packages to the newest R installation from the one version before it (if ask=T, it will ask you between which two versions to perform the copying)
library('ahp')
AND <- c(rep(0,7),1)
OR <- c(0,rep(1,7))
binary.data <- data.frame(expand.grid(c(0,1), c(0,1), c(0,1)), AND, OR)
print(net <- neuralnet(AND+OR~Var1+Var2+Var3, binary.data, hidden=0,
rep=10, err.fct="ce", linear.output=FALSE))
XOR <- c(0,1,1,0)
xor.data <- data.frame(expand.grid(c(0,1), c(0,1)), XOR)
print(net.xor <- neuralnet(XOR~Var1+Var2, xor.data, hidden=2, rep=5))
plot(net.xor, rep="best")
data(infert, package="datasets")
print(net.infert <- neuralnet(case~parity+induced+spontaneous, infert,
err.fct="ce", linear.output=FALSE, likelihood=TRUE))
gwplot(net.infert, selected.covariate="parity")
gwplot(net.infert, selected.covariate="induced")
gwplot(net.infert, selected.covariate="spontaneous")
confidence.interval(net.infert)
Var1 <- runif(50, 0, 100)
sqrt.data <- data.frame(Var1, Sqrt=sqrt(Var1))
print(net.sqrt <- neuralnet(Sqrt~Var1, sqrt.data, hidden=10,
threshold=0.01))
compute(net.sqrt, (1:10)^2)$net.result
Var1 <- rpois(100,0.5)
Var2 <- rbinom(100,2,0.6)
Var3 <- rbinom(100,1,0.5)
SUM <- as.integer(abs(Var1+Var2+Var3+(rnorm(100))))
sum.data <- data.frame(Var1+Var2+Var3, SUM)
print(net.sum <- neuralnet(SUM~Var1+Var2+Var3, sum.data, hidden=1,
act.fct="tanh"))
prediction(net.sum)
install.packages('neuralnet')
library(neuralnet)
library('ahp')
library('neuralnet')
AND <- c(rep(0,7),1)
OR <- c(0,rep(1,7))
binary.data <- data.frame(expand.grid(c(0,1), c(0,1), c(0,1)), AND, OR)
print(net <- neuralnet(AND+OR~Var1+Var2+Var3, binary.data, hidden=0,
rep=10, err.fct="ce", linear.output=FALSE))
XOR <- c(0,1,1,0)
xor.data <- data.frame(expand.grid(c(0,1), c(0,1)), XOR)
print(net.xor <- neuralnet(XOR~Var1+Var2, xor.data, hidden=2, rep=5))
plot(net.xor, rep="best")
data(infert, package="datasets")
print(net.infert <- neuralnet(case~parity+induced+spontaneous, infert,
err.fct="ce", linear.output=FALSE, likelihood=TRUE))
gwplot(net.infert, selected.covariate="parity")
gwplot(net.infert, selected.covariate="induced")
gwplot(net.infert, selected.covariate="spontaneous")
confidence.interval(net.infert)
Var1 <- runif(50, 0, 100)
sqrt.data <- data.frame(Var1, Sqrt=sqrt(Var1))
print(net.sqrt <- neuralnet(Sqrt~Var1, sqrt.data, hidden=10,
threshold=0.01))
compute(net.sqrt, (1:10)^2)$net.result
Var1 <- rpois(100,0.5)
Var2 <- rbinom(100,2,0.6)
Var3 <- rbinom(100,1,0.5)
SUM <- as.integer(abs(Var1+Var2+Var3+(rnorm(100))))
sum.data <- data.frame(Var1+Var2+Var3, SUM)
print(net.sum <- neuralnet(SUM~Var1+Var2+Var3, sum.data, hidden=1,
act.fct="tanh"))
prediction(net.sum)
library("neuralnet")
#Going to create a neural network to perform the rooting of such a function: x^2+exp(-x)
#Type ?neuralnet for more information on the neuralnet library
#Generate 6 random numbers uniformly distributed between 0 and 100 (for very good results in such a range)
#And store them as a dataframe
traininginput <- as.data.frame(runif(6, min=0, max=30))
trainingoutput <- (traininginput^2)+exp(-traininginput)
#Column bind the data into one variable
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
#Train the neural network
#Going to have C(5, 3) hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
net.myfunc <- neuralnet(Output~Input, trainingdata, hidden=c(5, 3), threshold=0.01)
print(net.myfunc)
#Plot the neural network
plot(net.myfunc)
#Test the neural network on some training data
testdata <- as.data.frame((2:20)*0.5) #Generate some numbers between 1 and 10
net.results <- compute(net.myfunc, testdata) #Run them through the neural network
#Lets see what properties net.myfunc has
ls(net.results)
#Lets see the results
print(net.results$net.result)
#Lets display a better version of the results
cleanoutput <- cbind(testdata,(testdata^2)+exp(-testdata),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
library("neuralnet")
#Going to create a neural network to perform the rooting of such a function: x^2+exp(-x)
#Type ?neuralnet for more information on the neuralnet library
#Generate 6 random numbers uniformly distributed between 0 and 100 (for very good results in such a range)
#And store them as a dataframe
traininginput <- as.data.frame(runif(6, min=0, max=30))
trainingoutput <- log10(traininginput)
#Column bind the data into one variable
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
#Train the neural network
#Going to have C(5, 3) hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
net.myfunc <- neuralnet(Output~Input, trainingdata, hidden=c(5, 3), threshold=0.01)
print(net.myfunc)
#Plot the neural network
plot(net.myfunc)
#Test the neural network on some training data
testdata <- as.data.frame((2:20)*0.5) #Generate some numbers between 1 and 10
net.results <- compute(net.myfunc, testdata) #Run them through the neural network
#Lets see what properties net.myfunc has
ls(net.results)
#Lets see the results
print(net.results$net.result)
#Lets display a better version of the results
cleanoutput <- cbind(testdata,(testdata^2)+exp(-testdata),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
library("neuralnet")
#Going to create a neural network to perform the rooting of such a function: x^2+exp(-x)
#Type ?neuralnet for more information on the neuralnet library
#Generate 6 random numbers uniformly distributed between 0 and 100 (for very good results in such a range)
#And store them as a dataframe
traininginput <- as.data.frame(runif(10, min=1, max=10))
trainingoutput <- log10(traininginput)
#Column bind the data into one variable
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
#Train the neural network
#Going to have C(5, 3) hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
net.myfunc <- neuralnet(Output~Input, trainingdata, hidden=c(5, 3), threshold=0.01)
print(net.myfunc)
#Plot the neural network
plot(net.myfunc)
#Test the neural network on some training data
testdata <- as.data.frame((2:20)*0.5) #Generate some numbers between 1 and 10
net.results <- compute(net.myfunc, testdata) #Run them through the neural network
#Lets see what properties net.myfunc has
ls(net.results)
#Lets see the results
print(net.results$net.result)
#Lets display a better version of the results
cleanoutput <- cbind(testdata,(testdata^2)+exp(-testdata),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
library("neuralnet")
#Going to create a neural network to perform the rooting of such a function: x^2+exp(-x)
#Type ?neuralnet for more information on the neuralnet library
#Generate 6 random numbers uniformly distributed between 0 and 100 (for very good results in such a range)
#And store them as a dataframe
traininginput <- as.data.frame(runif(10, min=1, max=10))
trainingoutput <- log10(traininginput)
#Column bind the data into one variable
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
#Train the neural network
#Going to have C(5, 3) hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
net.myfunc <- neuralnet(Output~Input, trainingdata, hidden=c(5, 3), threshold=0.01)
print(net.myfunc)
#Plot the neural network
plot(net.myfunc)
#Test the neural network on some training data
testdata <- as.data.frame((2:20)*0.5) #Generate some numbers between 1 and 10
net.results <- compute(net.myfunc, testdata) #Run them through the neural network
#Lets see what properties net.myfunc has
ls(net.results)
#Lets see the results
print(net.results$net.result)
#Lets display a better version of the results
cleanoutput <- cbind(testdata,log10(testdata),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
library("neuralnet")
#Going to create a neural network to perform the rooting of such a function: x^2+exp(-x)
#Type ?neuralnet for more information on the neuralnet library
#Generate 6 random numbers uniformly distributed between 0 and 100 (for very good results in such a range)
#And store them as a dataframe
traininginput <- as.data.frame(runif(10, min=1, max=10))
trainingoutput <- log10(traininginput)
#Column bind the data into one variable
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
#Train the neural network
#Going to have C(5, 3) hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
net.myfunc <- neuralnet(Output~Input, trainingdata, hidden=c(5, 3), threshold=0.01)
print(net.myfunc)
#Plot the neural network
plot(net.myfunc)
#Test the neural network on some training data
testdata <- as.data.frame((1:10)*1) #Generate some numbers between 1 and 10
net.results <- compute(net.myfunc, testdata) #Run them through the neural network
#Lets see what properties net.myfunc has
ls(net.results)
#Lets see the results
print(net.results$net.result)
#Lets display a better version of the results
cleanoutput <- cbind(testdata,log10(testdata),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
library("neuralnet")
#Going to create a neural network to perform the rooting of such a function: x^2+exp(-x)
#Type ?neuralnet for more information on the neuralnet library
#Generate 6 random numbers uniformly distributed between 0 and 100 (for very good results in such a range)
#And store them as a dataframe
traininginput <- as.data.frame(runif(10, min=1, max=10))
trainingoutput <- (traininginput^(-0.25))
#Column bind the data into one variable
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
#Train the neural network
#Going to have C(5, 3) hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
net.myfunc <- neuralnet(Output~Input, trainingdata, hidden=c(5, 3), threshold=0.01)
print(net.myfunc)
#Plot the neural network
plot(net.myfunc)
#Test the neural network on some training data
testdata <- as.data.frame((1:10)*1) #Generate some numbers between 1 and 10
net.results <- compute(net.myfunc, testdata) #Run them through the neural network
#Lets see what properties net.myfunc has
ls(net.results)
#Lets see the results
print(net.results$net.result)
#Lets display a better version of the results
cleanoutput <- cbind(testdata,log10(testdata),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
library("neuralnet")
#Going to create a neural network to perform the rooting of such a function: x^2+exp(-x)
#Type ?neuralnet for more information on the neuralnet library
#Generate 6 random numbers uniformly distributed between 0 and 100 (for very good results in such a range)
#And store them as a dataframe
traininginput <- as.data.frame(runif(10, min=1, max=10))
trainingoutput <- (traininginput^(-0.25))
#Column bind the data into one variable
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
#Train the neural network
#Going to have C(5, 3) hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
net.myfunc <- neuralnet(Output~Input, trainingdata, hidden=c(5, 3), threshold=0.01)
print(net.myfunc)
#Plot the neural network
plot(net.myfunc)
#Test the neural network on some training data
testdata <- as.data.frame((1:10)*1) #Generate some numbers between 1 and 10
net.results <- compute(net.myfunc, testdata) #Run them through the neural network
#Lets see what properties net.myfunc has
ls(net.results)
#Lets see the results
print(net.results$net.result)
#Lets display a better version of the results
cleanoutput <- cbind(testdata,testdata^(-0.25),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
library("neuralnet")
#Going to create a neural network to perform the rooting of such a function: x^2+exp(-x)
#Type ?neuralnet for more information on the neuralnet library
#Generate 6 random numbers uniformly distributed between 0 and 100 (for very good results in such a range)
#And store them as a dataframe
traininginput <- as.data.frame(runif(10, min=1, max=10))
trainingoutput <- (traininginput^(-0.25))
#Column bind the data into one variable
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
#Train the neural network
#Going to have C(5, 3) hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
net.myfunc <- neuralnet(Output~Input, trainingdata, hidden=c(5, 3), threshold=0.01)
print(net.myfunc)
#Plot the neural network
plot(net.myfunc)
#Test the neural network on some training data
testdata <- as.data.frame((1:10)*1) #Generate some numbers between 1 and 10
net.results <- compute(net.myfunc, testdata) #Run them through the neural network
#Lets see what properties net.myfunc has
ls(net.results)
#Lets see the results
print(net.results$net.result)
#Lets display a better version of the results
cleanoutput <- cbind(testdata,testdata^(-0.25),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
library("neuralnet")
#Going to create a neural network to perform the rooting of such a function: x^2+exp(-x)
#Type ?neuralnet for more information on the neuralnet library
#Generate 6 random numbers uniformly distributed between 0 and 100 (for very good results in such a range)
#And store them as a dataframe
traininginput <- as.data.frame(runif(10, min=1, max=10))
trainingoutput <- (traininginput^(-0.25))
#Column bind the data into one variable
trainingdata <- cbind(traininginput,trainingoutput)
colnames(trainingdata) <- c("Input","Output")
#Train the neural network
#Going to have C(5, 3) hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
net.myfunc <- neuralnet(Output~Input, trainingdata, hidden=c(5, 3), threshold=0.01)
print(net.myfunc)
#Plot the neural network
plot(net.myfunc)
#Test the neural network on some training data
testdata <- as.data.frame((1:10)*1) #Generate some numbers between 1 and 10
net.results <- compute(net.myfunc, testdata) #Run them through the neural network
#Lets see what properties net.myfunc has
ls(net.results)
#Lets see the results
print(net.results$net.result)
#Lets display a better version of the results
cleanoutput <- cbind(testdata,testdata^(-0.25),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
